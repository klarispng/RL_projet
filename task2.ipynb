{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38625534",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f15ad67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import tensordict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745f218f",
   "metadata": {},
   "source": [
    "### Creating Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68b3a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "\n",
    "class AdvantageNet(nn.Sequential):\n",
    "    def __init__(self, state_dim, d_hid, n_actions):\n",
    "        super(AdvantageNet,self).__init__()\n",
    "\n",
    "        # takes as input the dimension of our state space\n",
    "        self.fc1 = nn.Linear(state_dim,d_hid)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(d_hid, d_hid)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Output the expected cumulative return for each action\n",
    "        self.fc3 = nn.Linear(d_hid, n_actions)\n",
    "        self.npExtr = NormalParamExtractor()\n",
    "\n",
    "\n",
    "class ValueNet(nn.Sequential):\n",
    "    def __init__(self, state_dim, d_hid):\n",
    "        super(ValueNet,self).__init__()\n",
    "\n",
    "        # takes as input the dimension of our state space\n",
    "        self.fc1 = nn.Linear(state_dim,d_hid)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(d_hid, d_hid)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Output the expected cumulative return for each action\n",
    "        self.fc3 = nn.Linear(d_hid, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26847f78",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49c4d205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.done = []\n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]\n",
    "        del self.done[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cffc647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, gamma):\n",
    "        self.env = gym.make(\"racetrack-v0\", render_mode=\"human\")\n",
    "        self.gamma = gamma\n",
    "        obs,_ = self.env.reset()\n",
    "        self.curr_obs = torch.tensor(obs.flatten())\n",
    "        \n",
    "        self.buffer = ReplayBuffer()\n",
    "\n",
    "        self.eps = 0.2\n",
    "\n",
    "        self.A = AdvantageNet(len(self.curr_obs), 256, 2)\n",
    "        self.old_A = AdvantageNet(len(self.curr_obs), 256, 2)\n",
    "        self.old_A.load_state_dict(self.A.state_dict())\n",
    "\n",
    "        self.V = ValueNet(len(self.curr_obs), 256)\n",
    "        self.old_V = ValueNet(len(self.curr_obs), 256)\n",
    "        self.old_V.load_state_dict(self.V.state_dict())\n",
    "\n",
    "        self.train_epochs = 5\n",
    "\n",
    "        self.update_freq = 5\n",
    "\n",
    "        self.n_episodes = 1000\n",
    "\n",
    "        self.A_optimizer = torch.optim.Adam(self.A.parameters(), lr=0.005)\n",
    "        self.V_optimizer = torch.optim.Adam(self.V.parameters(), lr=0.005)\n",
    "\n",
    "        self.V_criterion = nn.MSELoss()\n",
    "\n",
    "    def sample_action(self):\n",
    "        \n",
    "        mean, std = self.old_A(self.curr_obs)\n",
    "        dist = Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        state_val = self.old_V(self.curr_obs)\n",
    "        \n",
    "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
    "    \n",
    "    def evaluate_sa(self, state, action):\n",
    "        action_mean, action_std = self.A(state)\n",
    "        dist = Normal(action_mean, action_std)\n",
    "        \n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_val = self.V(state)\n",
    "\n",
    "        return action_logprobs, state_val, dist_entropy\n",
    "\n",
    "    def advantage_Monte_Carlo(self):\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, done in zip(reversed(self.buffer.rewards), reversed(self.buffer.done)):\n",
    "            if done:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + self.gamma*discounted_reward\n",
    "            rewards.insert(0,discounted_reward)\n",
    "\n",
    "        rewards = torch.tensor(rewards).reshape(-1,1)\n",
    "\n",
    "        value_estimations = torch.tensor(self.buffer.state_values).detach().reshape(-1,1)\n",
    "\n",
    "        advantage = rewards.detach() - value_estimations.detach()\n",
    "        advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "\n",
    "        return advantage, value_estimations\n",
    "\n",
    "\n",
    "    def compute_trajectory(self):\n",
    "        self.curr_obs, _ = self.env.reset()\n",
    "        self.curr_obs = torch.tensor(self.curr_obs).flatten()\n",
    "\n",
    "        done = 0\n",
    "        while not done:\n",
    "\n",
    "            action, action_log_prob, state_val = self.sample_action()\n",
    "\n",
    "            self.buffer.states.append(self.curr_obs.detach())\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_log_prob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            self.curr_obs, reward, terminated, truncated, _ = self.env.step(action.detach().numpy())\n",
    "            \n",
    "            done = terminated or truncated or not np.sum(self.curr_obs[1,:,:])\n",
    "            \n",
    "            self.curr_obs = torch.tensor(self.curr_obs).flatten()\n",
    "            \n",
    "            self.buffer.rewards.append(reward)\n",
    "            self.buffer.done.append(done)\n",
    "\n",
    "    def clip_update(self, logprob_ratio):\n",
    "        return torch.clip(logprob_ratio, 1-self.eps, 1+self.eps)\n",
    "\n",
    "    def update_network(self):\n",
    "        advantages, prev_state_values = self.advantage_Monte_Carlo()\n",
    "        \n",
    "        prev_states = torch.stack(self.buffer.states).detach()\n",
    "        prev_actions = torch.stack(self.buffer.actions).detach()\n",
    "        prev_logprobs = torch.stack(self.buffer.logprobs).detach()\n",
    "        \n",
    "\n",
    "        for _ in range(self.train_epochs):\n",
    "            logprobs, state_values, dist_entropy = self.evaluate_sa(prev_states, prev_actions)\n",
    "\n",
    "            logprobs_ratio = torch.exp(logprobs - prev_logprobs.detach())\n",
    "            A_loss_first_term = logprobs_ratio * advantages\n",
    "            A_loss_second_term = self.clip_update(logprobs_ratio) * advantages\n",
    "\n",
    "            loss_A = -torch.min(A_loss_first_term, A_loss_second_term)\n",
    "\n",
    "            loss_V = self.V_criterion(state_values, prev_state_values)\n",
    "\n",
    "            self.A_optimizer.zero_grad()\n",
    "            loss_A.mean().backward()\n",
    "            self.A_optimizer.step()\n",
    "\n",
    "            self.V_optimizer.zero_grad()\n",
    "            loss_V.mean().backward()\n",
    "            self.V_optimizer.step()\n",
    "\n",
    "        self.old_A.load_state_dict(self.A.state_dict())\n",
    "        self.old_V.load_state_dict(self.V.state_dict())\n",
    "\n",
    "        self.buffer.clear()\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for k in range(self.n_episodes):\n",
    "            self.compute_trajectory()\n",
    "\n",
    "            if k % self.update_freq == 0:\n",
    "                self.update_network()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fc403a07",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m ppo = PPO(\u001b[32m0.9\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mppo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 135\u001b[39m, in \u001b[36mPPO.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.n_episodes):\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m k % \u001b[38;5;28mself\u001b[39m.update_freq == \u001b[32m0\u001b[39m:\n\u001b[32m    138\u001b[39m             \u001b[38;5;28mself\u001b[39m.update_network()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mPPO.compute_trajectory\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mself\u001b[39m.buffer.logprobs.append(action_log_prob)\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m.buffer.state_values.append(state_val)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[38;5;28mself\u001b[39m.curr_obs, reward, terminated, truncated, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.sum(\u001b[38;5;28mself\u001b[39m.curr_obs[\u001b[32m1\u001b[39m,:,:])\n\u001b[32m     91\u001b[39m \u001b[38;5;28mself\u001b[39m.curr_obs = torch.tensor(\u001b[38;5;28mself\u001b[39m.curr_obs).flatten()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Etudes\\MASTER\\CentraleSupelec\\Cours\\RL\\proj\\RL_projet\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Etudes\\MASTER\\CentraleSupelec\\Cours\\RL\\proj\\RL_projet\\venv\\Lib\\site-packages\\gymnasium\\core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Etudes\\MASTER\\CentraleSupelec\\Cours\\RL\\proj\\RL_projet\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Etudes\\MASTER\\CentraleSupelec\\Cours\\RL\\proj\\RL_projet\\venv\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:242\u001b[39m, in \u001b[36mAbstractEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28mself\u001b[39m.time += \u001b[32m1\u001b[39m / \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mpolicy_frequency\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    240\u001b[39m \u001b[38;5;28mself\u001b[39m._simulate(action)\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m obs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobservation_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m reward = \u001b[38;5;28mself\u001b[39m._reward(action)\n\u001b[32m    244\u001b[39m terminated = \u001b[38;5;28mself\u001b[39m._is_terminated()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Etudes\\MASTER\\CentraleSupelec\\Cours\\RL\\proj\\RL_projet\\venv\\Lib\\site-packages\\highway_env\\envs\\common\\observation.py:400\u001b[39m, in \u001b[36mOccupancyGridObservation.observe\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    398\u001b[39m                 \u001b[38;5;28mself\u001b[39m.grid[layer, cell[\u001b[32m0\u001b[39m], cell[\u001b[32m1\u001b[39m]] = vehicle[feature]\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m feature == \u001b[33m\"\u001b[39m\u001b[33mon_road\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfill_road_layer_by_lanes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m obs = \u001b[38;5;28mself\u001b[39m.grid\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clip:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Etudes\\MASTER\\CentraleSupelec\\Cours\\RL\\proj\\RL_projet\\venv\\Lib\\site-packages\\highway_env\\envs\\common\\observation.py:471\u001b[39m, in \u001b[36mOccupancyGridObservation.fill_road_layer_by_lanes\u001b[39m\u001b[34m(self, layer_index, lane_perception_distance)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _to \u001b[38;5;129;01min\u001b[39;00m road.network.graph[_from].keys():\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m lane \u001b[38;5;129;01min\u001b[39;00m road.network.graph[_from][_to]:\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m         origin, _ = \u001b[43mlane\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobserver_vehicle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m         waypoints = np.arange(\n\u001b[32m    473\u001b[39m             origin - lane_perception_distance,\n\u001b[32m    474\u001b[39m             origin + lane_perception_distance,\n\u001b[32m    475\u001b[39m             lane_waypoints_spacing,\n\u001b[32m    476\u001b[39m         ).clip(\u001b[32m0\u001b[39m, lane.length)\n\u001b[32m    477\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m waypoint \u001b[38;5;129;01min\u001b[39;00m waypoints:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Etudes\\MASTER\\CentraleSupelec\\Cours\\RL\\proj\\RL_projet\\venv\\Lib\\site-packages\\highway_env\\road\\lane.py:359\u001b[39m, in \u001b[36mCircularLane.local_coordinates\u001b[39m\u001b[34m(self, position)\u001b[39m\n\u001b[32m    357\u001b[39m phi = np.arctan2(delta[\u001b[32m1\u001b[39m], delta[\u001b[32m0\u001b[39m])\n\u001b[32m    358\u001b[39m phi = \u001b[38;5;28mself\u001b[39m.start_phase + utils.wrap_to_pi(phi - \u001b[38;5;28mself\u001b[39m.start_phase)\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m r = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m longitudinal = \u001b[38;5;28mself\u001b[39m.direction * (phi - \u001b[38;5;28mself\u001b[39m.start_phase) * \u001b[38;5;28mself\u001b[39m.radius\n\u001b[32m    361\u001b[39m lateral = \u001b[38;5;28mself\u001b[39m.direction * (\u001b[38;5;28mself\u001b[39m.radius - r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Etudes\\MASTER\\CentraleSupelec\\Cours\\RL\\proj\\RL_projet\\venv\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2744\u001b[39m, in \u001b[36mnorm\u001b[39m\u001b[34m(x, ord, axis, keepdims)\u001b[39m\n\u001b[32m   2742\u001b[39m     sqnorm = x_real.dot(x_real) + x_imag.dot(x_imag)\n\u001b[32m   2743\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2744\u001b[39m     sqnorm = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2745\u001b[39m ret = sqrt(sqnorm)\n\u001b[32m   2746\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "ppo = PPO(0.9)\n",
    "ppo.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
