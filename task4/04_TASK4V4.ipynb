{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e015de9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../config/1-highway-discrete-config.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhighway_env\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Chargement de la configuration de base\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../config/1-highway-discrete-config.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     22\u001b[0m     base_config \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Création et configuration de l'environnement multi-agent\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------------\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../config/1-highway-discrete-config.pkl'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Multi-agent Double DQN sur l'environnement Highway-fast-v0 avec 2 véhicules contrôlés.\n",
    "Affiche les courbes de reward, reward moyen, et évolution d'epsilon et du learning rate pour chaque agent.\n",
    "\"\"\"\n",
    "\n",
    "import gymnasium as gym\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import highway_env\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Chargement de la configuration de base\n",
    "# --------------------------------------------------------------------------\n",
    "with open(\"../config/1-highway-discrete-config.pkl\", \"rb\") as f:\n",
    "    base_config = pickle.load(f)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Création et configuration de l'environnement multi-agent\n",
    "# --------------------------------------------------------------------------\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"human\")\n",
    "env.unwrapped.config.update(base_config)\n",
    "env.unwrapped.config.update({\n",
    "    \"controlled_vehicles\": 2,\n",
    "    \"vehicles_count\": 50,  # exemple : nombre total de véhicules\n",
    "    \"action\": {\n",
    "        \"type\": \"MultiAgentAction\",\n",
    "        \"action_config\": {\"type\": \"DiscreteMetaAction\"}\n",
    "    },\n",
    "    \"observation\": {\n",
    "        \"type\": \"MultiAgentObservation\",\n",
    "        \"observation_config\": {\"type\": \"Kinematics\"}\n",
    "    },\n",
    "    \"scaling\": 7.5,\n",
    "    \"centering_position\": [0.5, 0.5]\n",
    "})\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Fonctions d'aide\n",
    "# --------------------------------------------------------------------------\n",
    "def is_risky(state):\n",
    "    grid_range = base_config[\"observation\"][\"grid_size\"][0]\n",
    "    grid_step  = base_config[\"observation\"][\"grid_step\"][0]\n",
    "    nx = int((grid_range[1] - grid_range[0]) / grid_step)\n",
    "    ny = int((base_config[\"observation\"][\"grid_size\"][1][1] - base_config[\"observation\"][\"grid_size\"][1][0]) / base_config[\"observation\"][\"grid_step\"][1])\n",
    "    num_features = 7\n",
    "    if state.size != nx * ny * num_features:\n",
    "        return False\n",
    "    grid = state.reshape(nx, ny, num_features)\n",
    "    return np.any(grid[-1, :, 0] > 0.5)\n",
    "\n",
    "def shape_reward(reward, action, state):\n",
    "    shaped = reward\n",
    "    # renforcement des penalités de collision\n",
    "    if reward <= base_config.get(\"collision_reward\", -1):\n",
    "        shaped = reward * 2\n",
    "    # pénalité pour action risquée\n",
    "    if action == 1 and is_risky(state):\n",
    "        shaped -= 0.5\n",
    "    return shaped\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Réseau de neurones et agent DQN (Double DQN + masquage)\n",
    "# --------------------------------------------------------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.q_net     = DQN(state_dim, action_dim)\n",
    "        self.target_net= DQN(state_dim, action_dim)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=1e-3)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.9)\n",
    "        self.replay_buffer = deque(maxlen=100000)\n",
    "        self.gamma     = 0.99\n",
    "        self.epsilon   = 1.0\n",
    "        self.epsilon_min   = 0.05\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size    = 64\n",
    "        self.action_dim    = action_dim\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            acts = list(range(self.action_dim))\n",
    "            if is_risky(state) and 1 in acts:\n",
    "                acts.remove(1)\n",
    "            return random.choice(acts) if acts else 1\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_vals = self.q_net(state_t).squeeze(0).numpy()\n",
    "        if is_risky(state):\n",
    "            q_vals[1] = -np.inf\n",
    "        return int(np.argmax(q_vals))\n",
    "\n",
    "    def update_epsilon(self, ep, total_ep):\n",
    "        if ep > total_ep // 2:\n",
    "            self.epsilon = self.epsilon_min\n",
    "        else:\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        s, a, r, s2, d = map(np.array, zip(*batch))\n",
    "        s  = torch.FloatTensor(s)\n",
    "        a  = torch.LongTensor(a).unsqueeze(1)\n",
    "        r  = torch.FloatTensor(r).unsqueeze(1)\n",
    "        s2 = torch.FloatTensor(s2)\n",
    "        d  = torch.FloatTensor(d).unsqueeze(1)\n",
    "\n",
    "        q      = self.q_net(s).gather(1, a)\n",
    "        with torch.no_grad():\n",
    "            next_a = self.q_net(s2).argmax(dim=1, keepdim=True)\n",
    "            q_next = self.target_net(s2).gather(1, next_a)\n",
    "            target = r + self.gamma * (1 - d) * q_next\n",
    "\n",
    "        loss = F.mse_loss(q, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Entraînement multi-agent\n",
    "# --------------------------------------------------------------------------\n",
    "def train_multi_agents(agents, env, num_episodes=100, sync_target=10, collision_threshold=0.5):\n",
    "    rewards_hist = {i: [] for i in range(len(agents))}\n",
    "    eps_hist     = {i: [] for i in range(len(agents))}\n",
    "    lr_hist      = {i: [] for i in range(len(agents))}\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        states = [o.flatten() for o in obs]\n",
    "        ep_rewards = [0.0 for _ in agents]\n",
    "        done = False\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            actions = tuple(agent.get_action(s) for agent, s in zip(agents, states))\n",
    "            next_obs, raw_reward, term, trunc, infos = env.step(actions)\n",
    "\n",
    "            # --- Récupération du reward par agent ---\n",
    "            if isinstance(raw_reward, (list, tuple, np.ndarray)):\n",
    "                rewards = list(raw_reward)\n",
    "            elif \"agents_rewards\" in infos:\n",
    "                rewards = list(infos[\"agents_rewards\"])\n",
    "            else:\n",
    "                rewards = [\n",
    "                    env.unwrapped._agent_reward(actions[i], env.unwrapped.controlled_vehicles[i])\n",
    "                    for i in range(len(agents))\n",
    "                ]\n",
    "\n",
    "            # --- Collision inter-agent ---\n",
    "            pos = [veh.position for veh in env.unwrapped.controlled_vehicles]\n",
    "            dist = np.linalg.norm(np.array(pos[0]) - np.array(pos[1]))\n",
    "            if dist < collision_threshold:\n",
    "                print(f\"[Ep{ep} | Step{step}] Collision entre agents détectée (dist={dist:.2f})\")\n",
    "\n",
    "            next_states = [no.flatten() for no in next_obs]\n",
    "            done = term or trunc\n",
    "\n",
    "            # --- Stockage et apprentissage séparé ---\n",
    "            for i, agent in enumerate(agents):\n",
    "                sr = shape_reward(rewards[i], actions[i], states[i])\n",
    "                agent.replay_buffer.append((states[i], actions[i], sr, next_states[i], float(done)))\n",
    "                agent.update()\n",
    "                ep_rewards[i] += rewards[i]\n",
    "\n",
    "            states = next_states\n",
    "            step += 1\n",
    "\n",
    "        # --- Fin d'épisode : epsilon, scheduler, historiques ---\n",
    "        for i, agent in enumerate(agents):\n",
    "            agent.update_epsilon(ep, num_episodes)\n",
    "            agent.scheduler.step()\n",
    "            rewards_hist[i].append(ep_rewards[i])\n",
    "            eps_hist[i].append(agent.epsilon)\n",
    "            lr_hist[i].append(agent.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        if ep % sync_target == 0:\n",
    "            for agent in agents:\n",
    "                agent.target_net.load_state_dict(agent.q_net.state_dict())\n",
    "\n",
    "        if ep % 50 == 0:\n",
    "            epsilons = [f\"{a.epsilon:.3f}\" for a in agents]\n",
    "            print(f\"[Episode {ep}] Réwards: {ep_rewards}, Epsilons: {epsilons}\")\n",
    "\n",
    "    return rewards_hist, eps_hist, lr_hist\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Visualisation\n",
    "# --------------------------------------------------------------------------\n",
    "def plot_agent_metrics(hist, label, window=20):\n",
    "    plt.figure()\n",
    "    for i, vals in hist.items():\n",
    "        plt.plot(vals, label=f\"Agent {i+1}\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(label)\n",
    "    plt.legend()\n",
    "    plt.title(f\"Évolution de {label}\")\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Exécution principale\n",
    "# --------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Dimensions d'état et d'action\n",
    "    state_dim = obs[0].flatten().shape[0]\n",
    "    action_spaces = env.action_space.spaces if hasattr(env.action_space, 'spaces') else env.action_space\n",
    "    action_dims = [space.n for space in action_spaces]\n",
    "\n",
    "    # Création des agents\n",
    "    agents = [DQNAgent(state_dim, ad) for ad in action_dims]\n",
    "\n",
    "    # Entraînement\n",
    "    rewards_hist, eps_hist, lr_hist = train_multi_agents(agents, env, num_episodes=200, sync_target=10)\n",
    "\n",
    "    # Affichage des courbes\n",
    "    plot_agent_metrics(rewards_hist, \"Reward par épisode\")\n",
    "    # Reward moyen mobile\n",
    "    plt.figure()\n",
    "    window = 20\n",
    "    for i, r in rewards_hist.items():\n",
    "        mov = np.convolve(r, np.ones(window)/window, mode='valid')\n",
    "        plt.plot(range(window-1, len(r)), mov, label=f\"Agent {i+1}\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(f\"Reward moyen (fenêtre {window})\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Reward moyen sur 20 épisodes\")\n",
    "    plt.show()\n",
    "\n",
    "    plot_agent_metrics(eps_hist, \"Epsilon\")\n",
    "    plot_agent_metrics(lr_hist, \"Learning Rate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d2a45d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
