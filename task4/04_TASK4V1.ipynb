{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a18b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ccdeb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] Réwards: [1.3083847736625516, 1.3083847736625516], Epsilons: [0.995, 0.995]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Multi-agent Double DQN sur l'environnement Highway-fast-v0 avec 2 véhicules contrôlés.\n",
    "Affiche les courbes de reward, reward moyen, et évolution d'epsilon et du learning rate pour chaque agent.\n",
    "\"\"\"\n",
    "\n",
    "import gymnasium as gym\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import highway_env\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Chargement de la configuration de base (optionnel)\n",
    "# --------------------------------------------------------------------------\n",
    "with open(\"config/1-highway-discrete-config.pkl\", \"rb\") as f:\n",
    "    base_config = pickle.load(f)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Création et configuration de l'environnement multi-agent\n",
    "# --------------------------------------------------------------------------\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"human\")\n",
    "env.unwrapped.config.update(base_config)\n",
    "# Configuration multi-agent et options de rendu\n",
    "env.unwrapped.config.update({\n",
    "    \"controlled_vehicles\": 2,\n",
    "    # Pour pouvoir envoyer un tuple d'actions lors du step\n",
    "    \"action\": {\n",
    "        \"type\": \"MultiAgentAction\",\n",
    "        \"action_config\": {\"type\": \"DiscreteMetaAction\"}\n",
    "    },\n",
    "    # Pour avoir un tuple d'observations par agent\n",
    "    \"observation\": {\n",
    "        \"type\": \"MultiAgentObservation\",\n",
    "        \"observation_config\": {\"type\": \"Kinematics\"}\n",
    "    },\n",
    "    # Ajustements graphiques : centrer la vue et échelle pour voir les deux véhicules simultanément\n",
    "    \"scaling\": 7.5,\n",
    "    \"centering_position\": [0.5, 0.5]\n",
    "})\n",
    "env.unwrapped.config.update(base_config)\n",
    "env.unwrapped.config.update({\n",
    "    \"controlled_vehicles\": 2,\n",
    "    \"observation\": {\n",
    "        \"type\": \"MultiAgentObservation\",\n",
    "        \"observation_config\": {\"type\": \"Kinematics\"}\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"MultiAgentAction\",\n",
    "        \"action_config\": {\"type\": \"DiscreteMetaAction\"}\n",
    "    }\n",
    "})\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Fonctions d'aide (mêmes que pour l'agent simple)\n",
    "# --------------------------------------------------------------------------\n",
    "def is_risky(state):\n",
    "    grid_range = base_config[\"observation\"][\"grid_size\"][0]\n",
    "    grid_step = base_config[\"observation\"][\"grid_step\"][0]\n",
    "    nx = int((grid_range[1] - grid_range[0]) / grid_step)\n",
    "    ny = int((base_config[\"observation\"][\"grid_size\"][1][1] - base_config[\"observation\"][\"grid_size\"][1][0]) / base_config[\"observation\"][\"grid_step\"][1])\n",
    "    num_features = 7\n",
    "    if state.size != nx * ny * num_features:\n",
    "        return False\n",
    "    grid = state.reshape(nx, ny, num_features)\n",
    "    return np.any(grid[-1, :, 0] > 0.5)\n",
    "\n",
    "def shape_reward(reward, action, state):\n",
    "    shaped = reward\n",
    "    if reward <= base_config.get(\"collision_reward\", -1):\n",
    "        shaped = reward * 2\n",
    "    if action == 1 and is_risky(state):\n",
    "        shaped -= 0.5\n",
    "    return shaped\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Réseau de neurones et agent DQN (Double DQN + masquage)\n",
    "# --------------------------------------------------------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.q_net = DQN(state_dim, action_dim)\n",
    "        self.target_net = DQN(state_dim, action_dim)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=1e-3)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.9)\n",
    "        self.replay_buffer = deque(maxlen=100000)\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 64\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            acts = list(range(self.action_dim))\n",
    "            if is_risky(state) and 1 in acts:\n",
    "                acts.remove(1)\n",
    "            return random.choice(acts) if acts else 1\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_vals = self.q_net(state_t).squeeze(0).numpy()\n",
    "        if is_risky(state):\n",
    "            q_vals[1] = -np.inf\n",
    "        return int(np.argmax(q_vals))\n",
    "\n",
    "    def update_epsilon(self, ep, total_ep):\n",
    "        if ep > total_ep // 2:\n",
    "            self.epsilon = self.epsilon_min\n",
    "        else:\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        s, a, r, s2, d = map(np.array, zip(*batch))\n",
    "        s  = torch.FloatTensor(s)\n",
    "        a  = torch.LongTensor(a).unsqueeze(1)\n",
    "        r  = torch.FloatTensor(r).unsqueeze(1)\n",
    "        s2 = torch.FloatTensor(s2)\n",
    "        d  = torch.FloatTensor(d).unsqueeze(1)\n",
    "\n",
    "        q      = self.q_net(s).gather(1, a)\n",
    "        with torch.no_grad():\n",
    "            next_a = self.q_net(s2).argmax(dim=1, keepdim=True)\n",
    "            q_next = self.target_net(s2).gather(1, next_a)\n",
    "            target = r + self.gamma * (1 - d) * q_next\n",
    "\n",
    "        loss = F.mse_loss(q, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Entraînement multi-agent\n",
    "# --------------------------------------------------------------------------\n",
    "def train_multi_agents(agents, env, num_episodes=1000, sync_target=10):\n",
    "    rewards_hist = {i: [] for i in range(len(agents))}\n",
    "    eps_hist     = {i: [] for i in range(len(agents))}\n",
    "    lr_hist      = {i: [] for i in range(len(agents))}\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        states = [o.flatten() for o in obs]\n",
    "        ep_rewards = [0.0 for _ in agents]\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            actions = tuple(agent.get_action(s) for agent, s in zip(agents, states))\n",
    "            next_obs, reward, term, trunc, infos = env.step(actions)\n",
    "            # Répéter le même reward pour chaque agent\n",
    "            rewards = [reward] * len(agents)\n",
    "            done = term or trunc\n",
    "            next_states = [no.flatten() for no in next_obs]\n",
    "\n",
    "            for i, agent in enumerate(agents):\n",
    "                sr = shape_reward(rewards[i], actions[i], states[i])\n",
    "                agent.replay_buffer.append((states[i], actions[i], sr, next_states[i], float(done)))\n",
    "                agent.update()\n",
    "                ep_rewards[i] += rewards[i]\n",
    "\n",
    "            states = next_states\n",
    "\n",
    "        for i, agent in enumerate(agents):\n",
    "            agent.update_epsilon(ep, num_episodes)\n",
    "            agent.scheduler.step()\n",
    "            rewards_hist[i].append(ep_rewards[i])\n",
    "            eps_hist[i].append(agent.epsilon)\n",
    "            lr_hist[i].append(agent.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        if ep % sync_target == 0:\n",
    "            for agent in agents:\n",
    "                agent.target_net.load_state_dict(agent.q_net.state_dict())\n",
    "\n",
    "        if ep % 50 == 0:\n",
    "            print(f\"[Episode {ep}] Réwards: {ep_rewards}, Epsilons: {[a.epsilon for a in agents]}\")\n",
    "\n",
    "    return rewards_hist, eps_hist, lr_hist\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Visualisation\n",
    "# --------------------------------------------------------------------------\n",
    "def plot_agent_metrics(hist, label, window=20):\n",
    "    plt.figure()\n",
    "    for i, vals in hist.items():\n",
    "        plt.plot(vals, label=f\"Agent {i+1}\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(label)\n",
    "    plt.legend()\n",
    "    plt.title(f\"Évolution de {label}\")\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Exécution principale\n",
    "# --------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    state_dim = obs[0].flatten().shape[0]\n",
    "    action_spaces = env.action_space.spaces if hasattr(env.action_space, 'spaces') else env.action_space\n",
    "    action_dims = [space.n for space in action_spaces]\n",
    "\n",
    "    agents = [DQNAgent(state_dim, ad) for ad in action_dims]\n",
    "    rewards_hist, eps_hist, lr_hist = train_multi_agents(agents, env)\n",
    "\n",
    "    # Affichage des courbes\n",
    "    window = 20\n",
    "    plot_agent_metrics(rewards_hist, \"Reward par épisode\")\n",
    "    # Moyenne mobile\n",
    "    for i, r in rewards_hist.items():\n",
    "        mov = np.convolve(r, np.ones(window)/window, mode='valid')\n",
    "        plt.plot(range(window-1, len(r)), mov, label=f\"Agent {i+1}\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(f\"Reward moyen (fenêtre {window})\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Reward moyen sur 20 épisodes\")\n",
    "    plt.show()\n",
    "\n",
    "    plot_agent_metrics(eps_hist, \"Epsilon\")\n",
    "    plot_agent_metrics(lr_hist, \"Learning Rate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd82931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
