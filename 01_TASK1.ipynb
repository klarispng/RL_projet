{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de610a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import highway_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "161f035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/1-highway-discrete-config.pkl\", \"rb\") as f:\n",
    "    config = pickle.load(f)\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"human\")\n",
    "env.unwrapped.configure(config)\n",
    "observation, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a351afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59eaf741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0):\n",
    "        self.q_net = DQN(state_dim, action_dim)\n",
    "        self.target_net = DQN(state_dim, action_dim)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "        self.replay_buffer = deque(maxlen=100000)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            return self.q_net(state).argmax().item()\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            max_next_q = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "            target = rewards + self.gamma * (1 - dones) * max_next_q\n",
    "\n",
    "        loss = F.mse_loss(q_values, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f15d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, episodes=1000, sync_target_every=10):\n",
    "    all_rewards = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = state.flatten()  # Adapter si besoin\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = next_state.flatten()\n",
    "            done = terminated or truncated\n",
    "\n",
    "            agent.replay_buffer.append((state, action, reward, next_state, done))\n",
    "            agent.update()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        agent.epsilon = max(agent.epsilon * agent.epsilon_decay, agent.epsilon_min)\n",
    "        all_rewards.append(total_reward)\n",
    "\n",
    "        if ep % sync_target_every == 0:\n",
    "            agent.target_net.load_state_dict(agent.q_net.state_dict())\n",
    "\n",
    "        if ep % 10 == 0:\n",
    "            print(f\"Episode {ep}: Reward = {total_reward:.2f}\")\n",
    "\n",
    "    return all_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e751d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards):\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"Training Performance\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b47e9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Reward = 4.20\n",
      "Episode 10: Reward = 7.84\n",
      "Episode 20: Reward = 6.59\n",
      "Episode 30: Reward = 5.92\n",
      "Episode 40: Reward = 1.78\n",
      "Episode 50: Reward = 36.07\n",
      "Episode 60: Reward = 32.19\n",
      "Episode 70: Reward = 14.94\n",
      "Episode 80: Reward = 13.55\n",
      "Episode 90: Reward = 12.97\n",
      "Episode 100: Reward = 7.55\n",
      "Episode 110: Reward = 3.55\n",
      "Episode 120: Reward = 3.15\n",
      "Episode 130: Reward = 9.33\n",
      "Episode 140: Reward = 5.87\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(state_dim, action_dim)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Lancement de l'entraînement\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msync_target_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Affichage des résultats\u001b[39;00m\n\u001b[0;32m     14\u001b[0m plot_rewards(rewards)\n",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(agent, env, episodes, sync_target_every)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     11\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[1;32m---> 12\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m next_state\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     14\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\ccdeb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ccdeb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ccdeb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ccdeb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:240\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m     )\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    243\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Users\\ccdeb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:280\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    278\u001b[0m         frame \u001b[38;5;241m<\u001b[39m frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    279\u001b[0m     ):  \u001b[38;5;66;03m# Last frame will be rendered through env.render() as usual\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_automatic_rendering\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_auto_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ccdeb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:340\u001b[0m, in \u001b[0;36mAbstractEnv._automatic_rendering\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_video_wrapper\u001b[38;5;241m.\u001b[39mvideo_recorder\u001b[38;5;241m.\u001b[39mcapture_frame()\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ccdeb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:303\u001b[0m, in \u001b[0;36mAbstractEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;241m=\u001b[39m EnvViewer(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_auto_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39moffscreen:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39mhandle_events()\n",
      "File \u001b[1;32mc:\\Users\\ccdeb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\graphics.py:160\u001b[0m, in \u001b[0;36mEnvViewer.display\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal_time_rendering\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 160\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSAVE_IMAGES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory:\n\u001b[0;32m    163\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39msave(\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim_surface,\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhighway-env_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    166\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fonction principale\n",
    "if __name__ == \"__main__\":\n",
    "    # Détermination de la dimension de l'état et de celle de l'action\n",
    "    state_dim = observation.flatten().shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # Initialisation de l'agent\n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "    # Lancement de l'entraînement\n",
    "    rewards = train(agent, env, episodes=1000, sync_target_every=10)\n",
    "\n",
    "    # Affichage des résultats\n",
    "    plot_rewards(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e89b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
