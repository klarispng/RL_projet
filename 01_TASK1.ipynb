{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de610a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f035b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gymnasium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Charger la config\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gymnasium'"
     ]
    }
   ],
   "source": [
    "with open(\"config\\1-highway-discrete-config.pkl\", \"rb\") as f:\n",
    "    config = pickle.load(f)\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\")\n",
    "env.unwrapped.configure(config)\n",
    "observation, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a351afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eaf741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0):\n",
    "        self.q_net = DQN(state_dim, action_dim)\n",
    "        self.target_net = DQN(state_dim, action_dim)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "        self.replay_buffer = deque(maxlen=100000)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            return self.q_net(state).argmax().item()\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            max_next_q = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "            target = rewards + self.gamma * (1 - dones) * max_next_q\n",
    "\n",
    "        loss = F.mse_loss(q_values, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f15d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, episodes=1000, sync_target_every=10):\n",
    "    all_rewards = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = state.flatten()  # Adapter si besoin\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = next_state.flatten()\n",
    "            done = terminated or truncated\n",
    "\n",
    "            agent.replay_buffer.append((state, action, reward, next_state, done))\n",
    "            agent.update()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        agent.epsilon = max(agent.epsilon * agent.epsilon_decay, agent.epsilon_min)\n",
    "        all_rewards.append(total_reward)\n",
    "\n",
    "        if ep % sync_target_every == 0:\n",
    "            agent.target_net.load_state_dict(agent.q_net.state_dict())\n",
    "\n",
    "        if ep % 10 == 0:\n",
    "            print(f\"Episode {ep}: Reward = {total_reward:.2f}\")\n",
    "\n",
    "    return all_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e751d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards):\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"Training Performance\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b47e9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
