{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f640b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import highway_env\n",
    "\n",
    "# Chargement de la configuration\n",
    "with open(\"config/1-highway-discrete-config.pkl\", \"rb\") as f:\n",
    "    config = pickle.load(f)\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"human\")\n",
    "env.unwrapped.configure(config)\n",
    "observation, _ = env.reset()\n",
    "\n",
    "# Fonctions d'aide\n",
    "def is_risky(state):\n",
    "    \"\"\"\n",
    "    Détermine si l'état (observation) présente un risque pour l'accélération.\n",
    "    On considère que l'observation est issue d'une grille d'occupation.\n",
    "    \n",
    "    Utilise la configuration sous config[\"observation\"]:\n",
    "      - grid_size et grid_step se trouvent dans config[\"observation\"][\"grid_size\"] et [\"grid_step\"].\n",
    "    On considère risqué si, dans la colonne la plus à droite de la grille, \n",
    "    une cellule indique la présence d'un véhicule (valeur > 0.5).\n",
    "    \"\"\"\n",
    "    # Correction : accès aux clés imbriquées dans \"observation\"\n",
    "    grid_range = config[\"observation\"][\"grid_size\"][0]  # par exemple [-20, 20] pour l'axe x\n",
    "    grid_step = config[\"observation\"][\"grid_step\"][0]     # par exemple 5\n",
    "    # Calcul du nombre de cellules sur l'axe x et y\n",
    "    nx = int((grid_range[1] - grid_range[0]) / grid_step)\n",
    "    ny = int((config[\"observation\"][\"grid_size\"][1][1] - config[\"observation\"][\"grid_size\"][1][0]) / config[\"observation\"][\"grid_step\"][1])\n",
    "    \n",
    "    num_features = 7  # d'après la config : ['presence', 'x', 'y', 'vx', 'vy', 'cos_h', 'sin_h']\n",
    "    \n",
    "    # On s'assure que la taille de state est bien nx * ny * num_features\n",
    "    if state.size != nx * ny * num_features:\n",
    "        return False  # On considère par défaut que l'état est sûr si la taille ne correspond pas.\n",
    "    \n",
    "    grid = state.reshape(nx, ny, num_features)\n",
    "    # On considère \"le front\" comme la colonne la plus à droite (indice -1 sur l'axe x)\n",
    "    front_presence = grid[-1, :, 0]  # Feature 0 correspond à la présence\n",
    "    # Si une cellule du front indique la présence d'un véhicule (valeur supérieure à 0.5), on considère le contexte comme risqué.\n",
    "    if np.any(front_presence > 0.5):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def shape_reward(reward, action, state):\n",
    "    \"\"\"\n",
    "    Applique le reward shaping :\n",
    "      - Si la reward indique une collision (par exemple la valeur 'collision_reward' de la config),\n",
    "        on renforce la pénalité.\n",
    "      - Si l'action est d'accélérer (ici action == 1) dans un contexte risqué, on applique un malus supplémentaire.\n",
    "      - On ajoute ensuite un offset positif (ici +0.5) pour équilibrer le signal.\n",
    "    \"\"\"\n",
    "    shaped = reward\n",
    "    # 1. Renforcer la pénalisation des collisions.\n",
    "    if reward <= config.get(\"collision_reward\", -1):\n",
    "        shaped = reward * 2  # Pénalité plus sévère.\n",
    "    \n",
    "    # 2. Pénaliser l'accélération agressive si l'état est risqué.\n",
    "    if action == 1 and is_risky(state):\n",
    "        shaped -= 0.5  # Malus supplémentaire.\n",
    "    \n",
    "    return shaped\n",
    "\n",
    "# Définition du réseau DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Agent avec Double DQN et action masking\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0, batch_size=64):\n",
    "        self.q_net = DQN(state_dim, action_dim)\n",
    "        self.target_net = DQN(state_dim, action_dim)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.9)\n",
    "\n",
    "        self.replay_buffer = deque(maxlen=100000)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = batch_size\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # En mode exploration, on applique un filtrage sur les actions dangereuses.\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            available_actions = list(range(self.action_dim))\n",
    "            # Action masking : si l'état est risqué, retirer l'action \"accélérer\" (index 1) de la sélection.\n",
    "            if is_risky(state) and 1 in available_actions:\n",
    "                available_actions.remove(1)\n",
    "                if len(available_actions) == 0:\n",
    "                    available_actions = [1]\n",
    "            return random.choice(available_actions)\n",
    "\n",
    "        # En mode exploitation, on ajuste les Q-values pour éviter l'accélération si l'état est risqué.\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state_tensor).squeeze(0).numpy()\n",
    "        if is_risky(state):\n",
    "            q_values[1] = -float('inf')\n",
    "        return int(np.argmax(q_values))\n",
    "    \n",
    "    def update_epsilon(self, current_episode, total_episodes):\n",
    "        \"\"\"\n",
    "        Met à jour epsilon avec décroissance habituelle.\n",
    "        Après la moitié des épisodes, on fixe epsilon à sa valeur minimale.\n",
    "        \"\"\"\n",
    "        if current_episode > total_episodes // 2:\n",
    "            self.epsilon = self.epsilon_min\n",
    "        else:\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "        # Calcul des Q-values actuels sur le réseau principal\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Double DQN :\n",
    "            # 1. Sélection de l'action optimale pour l'état suivant avec le réseau principal.\n",
    "            next_actions = self.q_net(next_states).argmax(dim=1, keepdim=True)\n",
    "            # 2. Évaluation de ces actions avec le réseau cible.\n",
    "            next_q_values = self.target_net(next_states).gather(1, next_actions)\n",
    "            target = rewards + self.gamma * (1 - dones) * next_q_values\n",
    "\n",
    "        loss = F.mse_loss(q_values, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # (Optionnel) Le gradient clipping peut être ajouté ici.\n",
    "        # torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "def train(agent, env, episodes=200, sync_target_every=10):\n",
    "    all_rewards = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = state.flatten()  # Ajuster la forme si nécessaire\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = next_state.flatten()\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Appliquer le reward shaping (en prenant en compte l'action et l'état)\n",
    "            shaped_reward = shape_reward(reward, action, state)\n",
    "            agent.replay_buffer.append((state, action, shaped_reward, next_state, done))\n",
    "            agent.update()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward  # Pour le suivi, on cumule la reward originale\n",
    "\n",
    "        agent.update_epsilon(ep, episodes)\n",
    "        agent.scheduler.step()\n",
    "        all_rewards.append(total_reward)\n",
    "\n",
    "        if ep % sync_target_every == 0:\n",
    "            agent.target_net.load_state_dict(agent.q_net.state_dict())\n",
    "\n",
    "        current_lr = agent.optimizer.param_groups[0]['lr']\n",
    "        if ep % 10 == 0:\n",
    "            print(f\"Episode {ep}: Reward = {total_reward:.2f}, Epsilon = {agent.epsilon:.4f}, LR = {current_lr:.6f}\")\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "def plot_rewards(rewards):\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"Training Performance\")\n",
    "    plt.show()\n",
    "\n",
    "# Fonction principale\n",
    "if __name__ == \"__main__\":\n",
    "    # Détermination de la dimension de l'état et du nombre d'actions\n",
    "    state_dim = observation.flatten().shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_dim, action_dim, lr=1e-3, batch_size=64)\n",
    "    rewards = train(agent, env, episodes=10, sync_target_every=10)\n",
    "    plot_rewards(rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb7c7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
