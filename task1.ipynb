{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6261a713",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b95d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import gymnasium as gym\n",
    "import highway_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b4964",
   "metadata": {},
   "source": [
    "### Setting up environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74ed486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making highway environment\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"human\")\n",
    "\n",
    "# Importing config\n",
    "with open('1-highway-discrete-config.pkl', 'rb') as pf:\n",
    "    config_dict = pickle.load(pf)\n",
    "env.unwrapped.configure(config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b9b8a6",
   "metadata": {},
   "source": [
    "### Creating DQN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bab79453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DQNNet(nn.Sequential):\n",
    "    def __init__(self, state_dim, d_hid, n_actions):\n",
    "        super(DQNNet,self).__init__()\n",
    "\n",
    "        # takes as input the dimension of our state space\n",
    "        self.fc1 = nn.Linear(state_dim,d_hid)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(d_hid, d_hid)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Output the expected cumulative return for each action\n",
    "        self.fc3 = nn.Linear(d_hid, n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14cefc2",
   "metadata": {},
   "source": [
    "### Creating Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03da6b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, maxsize):\n",
    "        self.buffer = deque([], maxlen=maxsize)\n",
    "        self.max_size = maxsize\n",
    "\n",
    "    def push(self, s, a, r, sn, done):\n",
    "        self.buffer.append((s,a,r,sn, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, min(len(self.buffer), batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f541e0",
   "metadata": {},
   "source": [
    "### Creating DQN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4f12edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "class DQN:\n",
    "    def __init__(self, env, buffer_size):\n",
    "\n",
    "        self.env = env\n",
    "        obs, info = self.env.reset()\n",
    "        self.curr_obs = torch.tensor(obs.flatten())\n",
    "        self.action_space = self.env.action_space\n",
    "        self.q_net = DQNNet(len(self.curr_obs.flatten()), 128, self.action_space.n)\n",
    "        self.target_net = DQNNet(len(self.curr_obs.flatten()), 128, self.action_space.n)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.replay_buffer = ReplayBuffer(maxsize = 1000)\n",
    "        self.gamma = 0.95\n",
    "        self.batchsize = 32\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters())\n",
    "        self.epsilon = 0.5\n",
    "\n",
    "    def sample_action(self, epsilon):\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            return self.action_space.sample()\n",
    "        else:\n",
    "            return torch.argmax(self.q_net(self.curr_obs))\n",
    "    \n",
    "    def update_buffer(self,s,a,r,sn, done):\n",
    "        s_torch = torch.tensor(s.flatten())\n",
    "        a_torch = torch.tensor(a)\n",
    "        r_torch = torch.tensor(r)\n",
    "        sn_torch = torch.tensor(sn.flatten())\n",
    "        done_torch = torch.tensor(done,dtype=torch.float32)\n",
    "        self.replay_buffer.push(s_torch,a_torch,r_torch,sn_torch,done_torch)\n",
    "        \n",
    "    \n",
    "    def compute_target(self, done, reward, obs):\n",
    "        \n",
    "        return reward + (1-done) * self.gamma * torch.max(self.target_net(obs))\n",
    "\n",
    "    def sample_minibatch(self, batchsize):      \n",
    "        batch = self.replay_buffer.sample(batchsize)\n",
    "        batch_curr_obs, batch_a, batch_r, batch_obs, batch_done = map(list, zip(*batch))\n",
    "        \n",
    "        return torch.tensor(batch_curr_obs).reshape(batchsize,-1), torch.tensor(batch_a).reshape(batchsize,-1),torch.tensor(batch_r).reshape(batchsize,-1),torch.tensor(batch_obs).reshape(batchsize,-1),torch.tensor(batch_done).reshape(batchsize,-1)\n",
    "    \n",
    "    def step(self):\n",
    "        \n",
    "        # Sample an action\n",
    "        a = self.sample_action(self.epsilon)\n",
    "        \n",
    "        # Perform one transition\n",
    "        obs, reward, done, truncated, _ = self.env.step(a)\n",
    "        \n",
    "        # Update the replay buffer\n",
    "        self.update_buffer(self.curr_obs, a, reward, obs, done)\n",
    "        \n",
    "        # Sample a minibatch of transitions from the replay buffer\n",
    "        self.curr_obs = torch.tensor(obs.flatten())\n",
    "        batch_curr_obs, batch_a, batch_r, batch_obs, batch_done = self.sample_minibatch(self.batchsize)\n",
    "        \n",
    "        # Compute the target\n",
    "        with torch.no_grad():\n",
    "            ys = self.compute_target(batch_done, batch_r, batch_obs)\n",
    "        \n",
    "        # Compute the predictions on the return\n",
    "        preds = self.q_net(batch_curr_obs).gather(1, batch_a)\n",
    "\n",
    "        loss = self.criterion(preds, ys)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return done\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        done = 0\n",
    "        for episode in tqdm(range(1, 500)):\n",
    "            \n",
    "            self.curr_obs, _ = self.env.reset()\n",
    "            self.curr_obs = torch.tensor(self.curr_obs.flatten())\n",
    "            while not done:\n",
    "                done = self.step()\n",
    "\n",
    "            if episode % 10 == 0:\n",
    "                self.curr_obs = env.reset()\n",
    "\n",
    "                done = 0\n",
    "                cum_ret = 0\n",
    "                it = 0\n",
    "                with torch.no_grad():\n",
    "                    self.q_net.eval()\n",
    "                    \n",
    "                    while not done:\n",
    "                \n",
    "                        a = self.sample_action(epsilon=0)\n",
    "                        self.curr_obs, reward, done, truncated, _ = self.env.step(a)\n",
    "                        cum_ret += self.gamma**it * reward\n",
    "                        it += 1\n",
    "\n",
    "                    self.q_net.train()\n",
    "                print(f'Reward on a test environnement at episode {episode}: {cum_ret}')\n",
    "\n",
    "                self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ecad14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(env=env, buffer_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "580b9ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/499 [00:00<?, ?it/s]C:\\Users\\arthu\\AppData\\Local\\Temp\\ipykernel_11872\\1066517751.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s_torch = torch.tensor(s.flatten())\n",
      "  0%|          | 0/499 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mDQN.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mself\u001b[39m.curr_obs = torch.tensor(\u001b[38;5;28mself\u001b[39m.curr_obs.flatten())\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     done = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m episode % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28mself\u001b[39m.curr_obs = env.reset()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mDQN.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Sample a minibatch of transitions from the replay buffer\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mself\u001b[39m.curr_obs = torch.tensor(obs.flatten())\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m batch_curr_obs, batch_a, batch_r, batch_obs, batch_done = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msample_minibatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Compute the target\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mDQN.sample_minibatch\u001b[39m\u001b[34m(self, batchsize)\u001b[39m\n\u001b[32m     41\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.replay_buffer.sample(batchsize)\n\u001b[32m     42\u001b[39m batch_curr_obs, batch_a, batch_r, batch_obs, batch_done = \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mzip\u001b[39m(*batch))\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_curr_obs\u001b[49m\u001b[43m)\u001b[49m.reshape(batchsize,-\u001b[32m1\u001b[39m), torch.tensor(batch_a).reshape(batchsize,-\u001b[32m1\u001b[39m),torch.tensor(batch_r).reshape(batchsize,-\u001b[32m1\u001b[39m),torch.tensor(batch_obs).reshape(batchsize,-\u001b[32m1\u001b[39m),torch.tensor(batch_done).reshape(batchsize,-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "dqn.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
