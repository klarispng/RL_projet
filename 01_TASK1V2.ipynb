{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f6ac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Reward = 11.32, Epsilon = 0.9950\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import highway_env\n",
    "\n",
    "# Chargement de la configuration\n",
    "with open(\"config/1-highway-discrete-config.pkl\", \"rb\") as f:\n",
    "    config = pickle.load(f)\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"human\")\n",
    "env.unwrapped.configure(config)\n",
    "observation, _ = env.reset()\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0):\n",
    "        self.q_net = DQN(state_dim, action_dim)\n",
    "        self.target_net = DQN(state_dim, action_dim)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "        self.replay_buffer = deque(maxlen=100000)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 64\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            return self.q_net(state).argmax().item()\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        # Mise à jour de l'epsilon avec décroissance, sans passer en dessous du seuil minimal\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            max_next_q = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "            target = rewards + self.gamma * (1 - dones) * max_next_q\n",
    "\n",
    "        loss = F.mse_loss(q_values, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "def train(agent, env, episodes=1000, sync_target_every=10):\n",
    "    all_rewards = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = state.flatten()  # Adapter la forme si nécessaire\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = next_state.flatten()\n",
    "            done = terminated or truncated\n",
    "\n",
    "            agent.replay_buffer.append((state, action, reward, next_state, done))\n",
    "            agent.update()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        # Mise à jour de l'epsilon après chaque épisode\n",
    "        agent.update_epsilon()\n",
    "        all_rewards.append(total_reward)\n",
    "\n",
    "        if ep % sync_target_every == 0:\n",
    "            agent.target_net.load_state_dict(agent.q_net.state_dict())\n",
    "\n",
    "        if ep % 10 == 0:\n",
    "            print(f\"Episode {ep}: Reward = {total_reward:.2f}, Epsilon = {agent.epsilon:.4f}\")\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "def plot_rewards(rewards):\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"Training Performance\")\n",
    "    plt.show()\n",
    "\n",
    "# Fonction principale\n",
    "if __name__ == \"__main__\":\n",
    "    # Détermination de la dimension de l'état et de celle de l'action\n",
    "    state_dim = observation.flatten().shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # Initialisation de l'agent\n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "    # Lancement de l'entraînement\n",
    "    rewards = train(agent, env, episodes=100, sync_target_every=5)\n",
    "\n",
    "    # Affichage des résultats\n",
    "    plot_rewards(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f640b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
